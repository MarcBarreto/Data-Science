{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtS5bd2S4iunMMi4q3eqK+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1j3_t2qz7Vc9"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torchvision\n","import torch.nn as nn\n","from torchvision import transforms\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","from torch import optim\n","from PIL import Image"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!7z x /content/drive/MyDrive/Colab\\ Notebooks/Data\\ Science/Learning/PyTorch/dogs-vs-cats.zip\n","!7z x /content/train.zip\n","!7z x /content/test1.zip "],"metadata":{"id":"qKPdMm_9n3Ki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","path = '/content/dogs-vs-cats'\n","os.mkdir(path)\n","os.rename('/content/train', os.path.join(path, 'train'))\n","os.rename('/content/test1', os.path.join(path, 'test1'))\n","os.mkdir(os.path.join(path, 'valid'))\n","files = glob.glob(os.path.join(path, 'train/*.jpg'))\n","no_of_images = len(files)\n","shuffle = np.random.permutation(no_of_images)\n","valid_size = int(3*int(no_of_images / 10))\n","for i in ['train', 'valid']:\n","  for folder in ['dog', 'cat']:\n","    os.mkdir(os.path.join(path, i, folder))\n","for i in shuffle[:valid_size]:\n","  image = files[i].split('/')[-1]\n","  folder = image.split('.')[0]\n","  os.rename(files[i], os.path.join(path, 'valid', folder, image))\n","for i in shuffle[valid_size:]:\n","  image = files[i].split('/')[-1]\n","  folder = image.split('.')[0]\n","  os.rename(files[i], os.path.join(path, 'train', folder, image))"],"metadata":{"id":"KnMln7gJn6pi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3),\n","])\n","valid_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","train_data = ImageFolder('/content/dogs-vs-cats/train', transform = train_transform)\n","valid_data = ImageFolder('/content/dogs-vs-cats/valid', transform = valid_transform)"],"metadata":{"id":"dKpRr9aQBB01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageLoader(Dataset):\n","  def __init__(self, dataset, transform = None):\n","    self.dataset = self.checkChannel(dataset)\n","    self.transform = transform\n","  def __len__(self):\n","    return len(self.dataset)\n","  def __getitem__(self, idx):\n","    image = Image.open(self.dataset[idx][0])\n","    classCategory = self.dataset[idx][1]\n","    if self.transform:\n","      image = self.transform(image)\n","    return image, classCategory\n","  def checkChannel(self, dataset):\n","    datasetRGB = []\n","    for index in range(len(dataset)):\n","      if (Image.open(dataset[index][0]).getbands() == ('R', 'G', 'B')):\n","        datasetRGB.append(dataset[index])\n","    return datasetRGB"],"metadata":{"id":"_je2mvhBSSoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n","valid_loader = DataLoader(valid_data, batch_size = 64, shuffle = True)"],"metadata":{"id":"xHrpTuwBO6KQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from torchvision.models import resnet50\n","model = resnet50(pretrained = True)\n","\n","for param in model.parameters():\n","  param.requires_grad = False\n","\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","model.to(device)"],"metadata":{"id":"vw_0xdzkWnOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.01)\n","\n","def train(num_epoch, model, train_loader):\n","  for epoch in range(0, num_epoch):\n","    current_loss = 0.0\n","    current_corrects = 0\n","    losses = []\n","    model.train()\n","    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n","    for batch_idx, (data, target) in loop:\n","      data = data.to(device = device)\n","      target = target.to(device = device)\n","      scores = model(data)\n","\n","      loss = criterion(scores, target)\n","      optimizer.zero_grad()\n","      losses.append(loss)\n","      loss.backward()\n","      optimizer.step()\n","      _, preds = torch.max(scores, 1)\n","      current_loss += loss.item()\n","      current_corrects += (preds == target).sum().item()\n","      accuracy = int(current_corrects / len(train_loader.dataset) * 100)\n","      loop.set_description(f'Epoch {epoch + 1} / {num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}')\n","      loop.set_postfix(loss = loss.data.item())\n","    print(f'Epoch: {epoch} Loss: {current_loss / len(train_loader.dataset)}. Accuracy: {current_corrects / len(train_loader.dataset)}')\n","    torch.save({\n","          'model_state_dict' : model.state_dict(),\n","          'optimizer_state_dict' : optimizer.state_dict(),\n","    }, 'checkpoint_epoch'+str(epoch) +'.pt')\n"],"metadata":{"id":"1mfQDEn3PoB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(model, valid_loader):\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  with torch.no_grad():\n","    for x, y in valid_loader:\n","      x = x.to(device)\n","      y = y.to(device)\n","      output = model(x)\n","      _, predictions = torch.max(output, 1)\n","      correct += (predictions == y).sum().item()\n","      test_loss += criterion(output, y)\n","  test_loss /= len(valid_loader.dataset) \n","  print('Average Loss:', test_loss, 'Accuracy:', correct, '/'), len(valid_loader.dataset), ' ', int(correct / len(valid_loader.dataset) * 100 ), '%'"],"metadata":{"id":"fhTJv9XPSnrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(5, model, train_loader)\n","test(model, valid_loader)"],"metadata":{"id":"GDglb64RT2CS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loading checkpoint\n","checkpoint = torch.load('/content/checkpoint_epoch4.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"metadata":{"id":"1BZ--IO5UGpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","path = '/content/dogs-vs-cats/test1'\n","files = glob.glob(os.path.join(path, '*.jpg'))\n","os.mkdir(os.path.join(path, 'test'))\n","\n","for i in range(0, len(files)):\n","  image = files[i].split('/')[-1]\n","  os.rename(files[i], os.path.join(path, 'test', image))\n","\n","test_dataset = ImageFolder('/content/dogs-vs-cats/test1', transform = test_transform)\n","test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"],"metadata":{"id":"hZB7nnrxUduy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, test_loader):\n","  with torch.no_grad():\n","    model.eval()\n","    for data, target in test_loader:\n","      data, target = data.to(device), target.to(device)\n","      output = model(data)\n","      _, pred = torch.max(output, 1)\n","      print(f'predict: {pred[0]}')"],"metadata":{"id":"3x8bklc7U6-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.01)\n","\n","def train(num_epoch, model, train_loader):\n","  for epoch in range(0, num_epoch):\n","    losses = []\n","    current_loss = 0.0\n","    current_accuracy = 0.0\n","    model.train()\n","    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n","    for batch_idx, (data, target) in loop:\n","      data, target = data.to(device), target.to(device)\n","      output = model(data)\n","      optimizer.zero_grad()\n","      loss = criterion(output, target)\n","      _, preds = torch.max(output, 1)\n","      model.backward()\n","      optimizer.step()\n","      losses.append(loss)\n","      current_loss += loss.item()\n","      current_accuracy += (preds == target).sum().item()\n","      loop.set_description(f'Epoch: {epoch + 1} / {num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}')\n","      loop.set_postfix(loss = loss.data.item())\n","    print(f'Epoch: {epoch} Loss: {current_loss / len(train_loader.dataset)}. Accuracy: {current_accuracy / len(train_loader.dataset)}')\n","    torch.save({\n","        'model_state_dict' : model.state_dict()\n","        'optimizer_state_dict' : optimizer.state_dict()\n","    }, 'checkpoint_epoch' + str(epoch) + '.pt')\n","\n","\n","def valid(num_epoch, model, valid_loader):\n","  model.eval()\n","  loss = 0\n","  correct = 0\n","  with torch.no_grad:\n","    for data, target in valid_loader:\n","      data, target = data.to(device), target.to(device)\n","      output = model(data)\n","      _, preds = torch.max(output, 1)\n","      correct += (preds == target).sum().item()\n","  loss = criterion(output, target)\n","\n","checkpoint = torch.load('/content/checkpoint_epoch0.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"metadata":{"id":"XQ_7aAzqtn1H"},"execution_count":null,"outputs":[]}]}