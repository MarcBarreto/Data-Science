{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-kMrCGuuvF_s"],"authorship_tag":"ABX9TyPEJxXi+rwm4agYGpQg6fld"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Install YoloV8(pip install)**"],"metadata":{"id":"-kMrCGuuvF_s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KxzLJGFujyf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680286416160,"user_tz":180,"elapsed":26304,"user":{"displayName":"Marcelo Barreto","userId":"14781669852484137175"}},"outputId":"4d8a175e-1b79-4383-f278-3da17c223bd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K\r\u001b[2K\rUltralytics YOLOv8.0.58 ðŸš€ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.3/78.2 GB disk)\n"]}],"source":["!pip install ultralytics\n","\n","from IPython.display import clear_output, Image\n","clear_output()\n","!yolo checks"]},{"cell_type":"markdown","source":["# **Import Data**"],"metadata":{"id":"wYsm9B6wsJLC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!7z x /content/drive/MyDrive/Colab\\ Notebooks/Data\\ Science/Learning/PyTorch/archive.zip"],"metadata":{"id":"0gD8ASMvurPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","dataset = pd.read_csv('/content/data/train_solution_bounding_boxes (1).csv')\n","dataset.head()"],"metadata":{"id":"GHy8s3i-vE1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.isnull().sum()"],"metadata":{"id":"vDMBoxv9phOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import os \n","\n","def normalize(labels, image_dir):\n","  for idx in labels.index:\n","    name = labels['image'][idx]\n","    image = os.path.join(image_dir, name)\n","    if os.path.isfile(image):\n","      img = Image.open(image)\n","      width, height = img.size\n","      labels['width'][idx] = labels['width'][idx] / width\n","      labels['height'][idx] = labels['height'][idx] / height\n","      labels['xc'][idx] = labels['xc'][idx] / width\n","      labels['yc'][idx] = labels['yc'][idx] / height\n"],"metadata":{"id":"h57H3pTumiRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['width'] = dataset.loc[:, 'xmax'] - dataset.loc[:, 'xmin']\n","dataset['height'] = dataset.loc[:, 'ymax'] - dataset.loc[:, 'ymin']\n","dataset['xc'] = dataset.loc[:, 'xmin'] + dataset.loc[:, 'width']/2\n","dataset['yc'] = dataset.loc[:, 'ymin'] + dataset.loc[:, 'height']/2\n","normalize(dataset, '/content/data/training_images')\n","dataset.head()"],"metadata":{"id":"EfTHpGm2rqy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.rename('/content/data/testing_images', '/content/data/test')\n","os.rename('/content/data/training_images', '/content/data/train')\n","file = open('/content/data/data.yaml', 'w')\n","file.writelines(\n","    'train: ../train/images' + '\\n' +\n","    'val: ../val/images' + '\\n' +\n","    'test: ../test/images' + '\\n' +\n","    'nc: 1' + '\\n' +\n","    \"names: ['car']\" + '\\n'\n",")\n","file.close()"],"metadata":{"id":"anC3g8ZPuY_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","images = dataset.loc[:]\n","images.drop_duplicates('image', keep = 'first', inplace = True)\n","X_train, X_valid, _, _ = train_test_split(images, images['xmin'], test_size = 0.2, random_state = 42)\n","X_train.head()"],"metadata":{"id":"9maKl_W6wsUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_path = '/content/data/train'\n","valid_path = '/content/data/val'\n","os.mkdir(training_path + '/images')\n","os.mkdir(training_path + '/labels')\n","os.mkdir(valid_path)\n","os.mkdir(valid_path + '/images')\n","os.mkdir(valid_path + '/labels')\n","\n","def move_images(dir_src, dir_dst, images):\n","  for i in images.index:\n","    image = images['image'][i]\n","    image_path = os.path.join(dir_src, image)\n","    os.rename(image_path, os.path.join(dir_dst, f'images/{image}'))\n","\n","def add_label(dir, images):\n"," for i in images.index:\n","   image = os.path.join(dir, images['image'][i])\n","   if os.path.isfile(image):\n","     label = image.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","     if os.path.isfile(label):\n","       with open(label, 'a') as f:\n","        f.write('\\n' + '0' + ' ' + str(images['xc'][i]) + ' ' + str(images['yc'][i]) + ' ' + str(images['width'][i]) + ' ' + str(images['height'][i]))\n","     else:\n","      with open(label, 'a') as f:\n","        f.write('0' + ' ' + str(images['xc'][i]) + ' ' + str(images['yc'][i]) + ' ' + str(images['width'][i]) + ' ' + str(images['height'][i]))"],"metadata":{"id":"6dDj0L5rJxRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["move_images(training_path, training_path, X_train)\n","move_images(training_path, valid_path, X_valid)\n","add_label(os.path.join(training_path, 'images'), dataset)\n","add_label(os.path.join(valid_path, 'images'), dataset)"],"metadata":{"id":"wbD5c6_BKl7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import numpy as np\n","\n","files = glob.glob('/content/data/train/vid_*.jpg')\n","num_of_images = len(files)\n","if num_of_images > 0:\n","  num = int(num_of_images * 5 / 100)\n","  shuffle = np.random.permutation(num_of_images)\n","  '''for i in shuffle[: num]:\n","    image = files[i].split('/')[-1]\n","    os.rename(files[i], f'/content/data/train/images/{image}')\n","    label = image.replace('.jpg', '.txt').replace('.png', '.txt')\n","    with open(os.path.join('/content/data/train/labels', label), 'w+') as f:\n","      f.close()'''\n","  for idx in shuffle:\n","    os.remove(files[idx])"],"metadata":{"id":"Oz5hG_B88YbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Model**"],"metadata":{"id":"hJDX5nbSsOtx"}},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","def set_res_dir(train = True):\n","  res_dir_count = len(glob.glob('/content/runs/detect/results_*'))\n","  if train:\n","    res_dir = f'results_{res_dir_count + 1}'\n","  else:\n","    res_dir = f'results_{res_dir_count}'\n","  return res_dir\n","\n","def train(model = 'yolov8n.pt', data = 'coco128.yaml', epochs = 25):\n","  res_dir = set_res_dir()\n","  model = YOLO(model = model)\n","  model.train(data = data, epochs = epochs, name = res_dir, imgsz = 640)\n","  return f'/content/runs/detect/{res_dir}/results.png', f'/content/runs/detect/{res_dir}/weights/best.pt'"],"metadata":{"id":"paFzaeZ1dLiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = [\n","    'yolov8n.pt',\n","    'yolov8s.pt',\n","    'yolov8m.pt',\n","    'yolov8l.pt',\n","    'yolov8x.pt'\n","]\n","data = '/content/data/data.yaml'\n","sources = r'/content/data/test/images/*.[jp][pn]g'\n","results = []\n","inferences = []\n","best_models = []\n","for model in models:\n","  result, best_model = train(model, data, 30)\n","  results.append(result)\n","  best_models.append(best_model)"],"metadata":{"id":"9PkK7gBdshBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference**"],"metadata":{"id":"lJhwHa34sVGx"}},{"cell_type":"code","source":["!wget https://app.roboflow.com/ds/s4gd3l9oss?key=pxE4Cs4MS4\n","!7z x /content/s4gd3l9oss?key=pxE4Cs4MS4"],"metadata":{"id":"f8g8VaEfUB2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchmetrics[detection]\n","\n","from torchmetrics.detection import mean_ap\n","\n","def meanAveragePrecision(predict, target):\n","  metric = mean_ap.MeanAveragePrecision(box_format = 'cxcywh')\n","  metric.update(predict, target)\n","  result = metric.compute()\n","  mAp = result['map']\n","  return mAp.numpy()"],"metadata":{"id":"v_VLJauisqBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","\n","def get_bboxes(labels):\n","  bboxes = []\n","  with open(labels, 'r') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","      values = line.split(' ')\n","      if len(values) < 4: break\n","      boxes = []\n","      boxes.append(float(values[1]))\n","      boxes.append(float(values[2])) \n","      boxes.append(float(values[3]))\n","      boxes.append(float(values[4].split('\\n')[0]))\n","      bboxes.append(boxes)\n","  return bboxes\n","\n","def desnormalize(bboxes, image):\n","  image = Image.open(image).convert('RGB')\n","  width, height = image.size\n","  for bbox in bboxes:\n","    xmin = bbox[0] - bbox[2]/2\n","    ymin = bbox[1] - bbox[3]/2\n","    xmax = bbox[0] + bbox[2]/2\n","    ymax = bbox[1] + bbox[3]/2\n","    bbox[0] = xmin * width\n","    bbox[1] = ymin * height\n","    bbox[2] = xmax * width\n","    bbox[3] = ymax * height\n","  return bboxes\n","\n","def inference(model_name, source):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  value = model_name.split('/')[-3].split('_')[-1]\n","  files = glob.glob(source + '/*.[jp][pn]g')\n","  results = []\n","  targets = []\n","  mAps = []\n","  for file in files:\n","    dir = file.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","    target = {}\n","    result = {}\n","    bboxes = get_bboxes(dir)\n","    target['boxes'] = torch.tensor(bboxes).to(device)\n","    target['labels'] = torch.zeros(len(target['boxes']), dtype = torch.int64).to(device)\n","    targets.append(target)\n","    image = file.split('/')[-1]\n","    inference_dir = f'inference_{value}/{image}'\n","    model = YOLO(model_name)\n","    output = model.predict(file, name = inference_dir, save = True, save_txt = True)\n","    result['boxes'] = output[0].boxes.xywhn.to(device)\n","    result['labels'] = torch.zeros(len(output[0].boxes), dtype = torch.int64).to(device)\n","    result['scores'] = output[0].boxes.cls.to(device)\n","    results.append(result)\n","    img_result = []\n","    img_target = []\n","    img_result.append(result)\n","    img_target.append(target)\n","    mAp = meanAveragePrecision(img_result, img_target)\n","    map_file = f'/content/runs/detect/{inference_dir}/map.txt'\n","    with open(map_file, 'a') as f:\n","      f.writelines(\n","          f'Image: {image}\\n mAp: {mAp}\\n Boxes: ' + '{\\n'\n","      )\n","      for box in target['boxes']:\n","        f.writelines(\n","            ' ' + str(box[0]) + ' ' + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + '\\n'\n","        )\n","      f.writelines('}')\n","    mAps.append(mAp)\n","  return results, targets, mAps\n","\n","def show_image(image, bboxes, save = False, dir = None):\n","  bboxes = get_bboxes(bboxes)\n","  bboxes = desnormalize(bboxes, image)\n","  img = np.array(Image.open(image).convert('RGB')).astype('float')\n","  for bbox in bboxes:\n","    img = cv2.rectangle(\n","        img,\n","        (int(bbox[0]), int(bbox[1])),\n","        (int(bbox[2]), int(bbox[3])),\n","        color = (255, 0, 0),\n","        thickness = 2\n","    )\n","  if save:\n","    if dir is None:\n","      dir = '/content/show_image'\n","      if not os.path.isdir(dir):\n","          os.mkdir(dir)\n","    name = image.split('/')[-1]\n","    cv2.imwrite(os.path.join(dir, name), img)\n","  else:\n","    cv2.imshow(img)"],"metadata":{"id":"nwVX5XuxsdWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_ground_truth(images, dir_dst):\n","  for img in glob.glob(f'{images}/*[jp][pn]g'):\n","    label = img.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","    img_name = img.split('/')[-1]\n","    dir = f'{dir_dst}/{img_name}/ground_truth'\n","    os.mkdir(dir)\n","    show_image(img, label, save = True, dir = dir)"],"metadata":{"id":"z0isDJprYS6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for img in glob.glob('/content/valid/images/*.[jp][pn]g'):\n","  dir_label = img.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","  with open(dir_label, 'r') as f:\n","    lines = f.readlines()\n","    if len(lines) == 0:\n","      os.remove(dir_label)\n","      os.remove(img)"],"metadata":{"id":"a6QW1ctSUQW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_list = []\n","targets_list = []\n","mAps_list = []\n","source = '/content/valid/images'\n","for best_model in best_models:\n","  results, targets, mAps = inference(best_model, source)\n","  value = best_model.split('/')[-3].split('_')[-1]\n","  dir_dst = f'/content/runs/detect/inference_{value}'\n","  set_ground_truth(source, dir_dst)\n","  results_list.append(results)\n","  targets_list.append(targets)\n","  mAps_list.append(mAps)"],"metadata":{"id":"-s_mr2r1V2p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","source = '/content/valid/images'\n","results, targets, maps = inference('/content/last.pt', source)"],"metadata":{"id":"jiP3oEHn4BhN"},"execution_count":null,"outputs":[]}]}