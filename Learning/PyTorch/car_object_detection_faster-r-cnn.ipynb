{"cells":[{"cell_type":"markdown","metadata":{"id":"wOPW8Uqe6dZP"},"source":["# **Download and Prepare Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhMnhD_VxOjC"},"outputs":[],"source":["from google.colab import drive\n","from IPython import display\n","\n","display.clear_output()\n","\n","drive.mount('/content/drive')\n","\n","!7z x /content/drive/MyDrive/Colab\\ Notebooks/Data\\ Science/Learning/PyTorch/archive.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZuEGuxXy6bh"},"outputs":[],"source":["import pandas as pd\n","\n","dataset = pd.read_csv('/content/data/train_solution_bounding_boxes (1).csv')\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxJWqYi-zTFY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680897420072,"user_tz":180,"elapsed":268,"user":{"displayName":"Marcelo Barreto","userId":"14781669852484137175"}},"outputId":"245a917d-210d-4cc9-893e-5fc28674ebc2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['vid_4_1000.jpg', 'vid_4_10000.jpg', 'vid_4_10040.jpg',\n","       'vid_4_10020.jpg', 'vid_4_10060.jpg', 'vid_4_10100.jpg',\n","       'vid_4_10120.jpg', 'vid_4_10140.jpg', 'vid_4_1020.jpg',\n","       'vid_4_1040.jpg', 'vid_4_10480.jpg', 'vid_4_10500.jpg',\n","       'vid_4_10520.jpg', 'vid_4_1060.jpg', 'vid_4_10960.jpg',\n","       'vid_4_10980.jpg', 'vid_4_11000.jpg', 'vid_4_11020.jpg',\n","       'vid_4_11240.jpg', 'vid_4_11260.jpg', 'vid_4_11280.jpg',\n","       'vid_4_11380.jpg', 'vid_4_11400.jpg', 'vid_4_11420.jpg',\n","       'vid_4_11440.jpg', 'vid_4_11900.jpg', 'vid_4_11880.jpg',\n","       'vid_4_11920.jpg', 'vid_4_11940.jpg', 'vid_4_11960.jpg',\n","       'vid_4_11980.jpg', 'vid_4_12000.jpg', 'vid_4_12040.jpg',\n","       'vid_4_12100.jpg', 'vid_4_12060.jpg', 'vid_4_12080.jpg',\n","       'vid_4_12120.jpg', 'vid_4_12140.jpg', 'vid_4_12160.jpg',\n","       'vid_4_12180.jpg', 'vid_4_12200.jpg', 'vid_4_12220.jpg',\n","       'vid_4_12240.jpg', 'vid_4_12260.jpg', 'vid_4_12280.jpg',\n","       'vid_4_12300.jpg', 'vid_4_12320.jpg', 'vid_4_12340.jpg',\n","       'vid_4_12360.jpg', 'vid_4_12380.jpg', 'vid_4_12480.jpg',\n","       'vid_4_13580.jpg', 'vid_4_13620.jpg', 'vid_4_13640.jpg',\n","       'vid_4_13660.jpg', 'vid_4_13680.jpg', 'vid_4_13700.jpg',\n","       'vid_4_13720.jpg', 'vid_4_13740.jpg', 'vid_4_13760.jpg',\n","       'vid_4_13780.jpg', 'vid_4_13800.jpg', 'vid_4_13840.jpg',\n","       'vid_4_13820.jpg', 'vid_4_13860.jpg', 'vid_4_13900.jpg',\n","       'vid_4_13880.jpg', 'vid_4_13920.jpg', 'vid_4_14140.jpg',\n","       'vid_4_14160.jpg', 'vid_4_14360.jpg', 'vid_4_14340.jpg',\n","       'vid_4_14380.jpg', 'vid_4_14400.jpg', 'vid_4_14460.jpg',\n","       'vid_4_14440.jpg', 'vid_4_14480.jpg', 'vid_4_14500.jpg',\n","       'vid_4_15000.jpg', 'vid_4_15020.jpg', 'vid_4_15040.jpg',\n","       'vid_4_16000.jpg', 'vid_4_16020.jpg', 'vid_4_16040.jpg',\n","       'vid_4_16060.jpg', 'vid_4_16100.jpg', 'vid_4_16120.jpg',\n","       'vid_4_16140.jpg', 'vid_4_16160.jpg', 'vid_4_16180.jpg',\n","       'vid_4_16280.jpg', 'vid_4_16320.jpg', 'vid_4_16300.jpg',\n","       'vid_4_16400.jpg', 'vid_4_16420.jpg', 'vid_4_16500.jpg',\n","       'vid_4_16660.jpg', 'vid_4_16680.jpg', 'vid_4_16700.jpg',\n","       'vid_4_16720.jpg', 'vid_4_17040.jpg', 'vid_4_17060.jpg',\n","       'vid_4_17080.jpg', 'vid_4_17100.jpg', 'vid_4_17140.jpg',\n","       'vid_4_17120.jpg', 'vid_4_17160.jpg', 'vid_4_17180.jpg',\n","       'vid_4_17240.jpg', 'vid_4_17260.jpg', 'vid_4_17320.jpg',\n","       'vid_4_17340.jpg', 'vid_4_17360.jpg', 'vid_4_17400.jpg',\n","       'vid_4_17440.jpg', 'vid_4_17420.jpg', 'vid_4_17560.jpg',\n","       'vid_4_17540.jpg', 'vid_4_17580.jpg', 'vid_4_1760.jpg',\n","       'vid_4_17600.jpg', 'vid_4_17620.jpg', 'vid_4_17640.jpg',\n","       'vid_4_17660.jpg', 'vid_4_17680.jpg', 'vid_4_1780.jpg',\n","       'vid_4_1800.jpg', 'vid_4_18180.jpg', 'vid_4_1820.jpg',\n","       'vid_4_18200.jpg', 'vid_4_18320.jpg', 'vid_4_18360.jpg',\n","       'vid_4_18340.jpg', 'vid_4_1840.jpg', 'vid_4_1860.jpg',\n","       'vid_4_1880.jpg', 'vid_4_18820.jpg', 'vid_4_18840.jpg',\n","       'vid_4_18860.jpg', 'vid_4_18880.jpg', 'vid_4_1900.jpg',\n","       'vid_4_19040.jpg', 'vid_4_19060.jpg', 'vid_4_19080.jpg',\n","       'vid_4_1920.jpg', 'vid_4_1940.jpg', 'vid_4_1960.jpg',\n","       'vid_4_19760.jpg', 'vid_4_19780.jpg', 'vid_4_19740.jpg',\n","       'vid_4_1980.jpg', 'vid_4_19880.jpg', 'vid_4_19900.jpg',\n","       'vid_4_19920.jpg', 'vid_4_2000.jpg', 'vid_4_2020.jpg',\n","       'vid_4_2040.jpg', 'vid_4_2060.jpg', 'vid_4_2080.jpg',\n","       'vid_4_2100.jpg', 'vid_4_21160.jpg', 'vid_4_21180.jpg',\n","       'vid_4_2120.jpg', 'vid_4_21200.jpg', 'vid_4_21220.jpg',\n","       'vid_4_21240.jpg', 'vid_4_21260.jpg', 'vid_4_21280.jpg',\n","       'vid_4_21300.jpg', 'vid_4_21320.jpg', 'vid_4_2140.jpg',\n","       'vid_4_21420.jpg', 'vid_4_21400.jpg', 'vid_4_21440.jpg',\n","       'vid_4_21480.jpg', 'vid_4_21500.jpg', 'vid_4_21520.jpg',\n","       'vid_4_21560.jpg', 'vid_4_21540.jpg', 'vid_4_21580.jpg',\n","       'vid_4_21620.jpg', 'vid_4_21600.jpg', 'vid_4_21640.jpg',\n","       'vid_4_2160.jpg', 'vid_4_21660.jpg', 'vid_4_21680.jpg',\n","       'vid_4_2180.jpg', 'vid_4_2200.jpg', 'vid_4_22240.jpg',\n","       'vid_4_22220.jpg', 'vid_4_22560.jpg', 'vid_4_22540.jpg',\n","       'vid_4_22580.jpg', 'vid_4_22640.jpg', 'vid_4_22660.jpg',\n","       'vid_4_22700.jpg', 'vid_4_22720.jpg', 'vid_4_22740.jpg',\n","       'vid_4_22980.jpg', 'vid_4_23000.jpg', 'vid_4_23020.jpg',\n","       'vid_4_2380.jpg', 'vid_4_2400.jpg', 'vid_4_2420.jpg',\n","       'vid_4_24760.jpg', 'vid_4_2520.jpg', 'vid_4_2540.jpg',\n","       'vid_4_26300.jpg', 'vid_4_26320.jpg', 'vid_4_26340.jpg',\n","       'vid_4_26360.jpg', 'vid_4_26380.jpg', 'vid_4_26420.jpg',\n","       'vid_4_26400.jpg', 'vid_4_26440.jpg', 'vid_4_26480.jpg',\n","       'vid_4_26500.jpg', 'vid_4_26460.jpg', 'vid_4_26520.jpg',\n","       'vid_4_26540.jpg', 'vid_4_26560.jpg', 'vid_4_26580.jpg',\n","       'vid_4_28220.jpg', 'vid_4_28200.jpg', 'vid_4_28240.jpg',\n","       'vid_4_28440.jpg', 'vid_4_28840.jpg', 'vid_4_28820.jpg',\n","       'vid_4_28880.jpg', 'vid_4_28860.jpg', 'vid_4_29460.jpg',\n","       'vid_4_29480.jpg', 'vid_4_29500.jpg', 'vid_4_29520.jpg',\n","       'vid_4_29540.jpg', 'vid_4_29560.jpg', 'vid_4_29880.jpg',\n","       'vid_4_29900.jpg', 'vid_4_29920.jpg', 'vid_4_29940.jpg',\n","       'vid_4_30000.jpg', 'vid_4_29980.jpg', 'vid_4_29960.jpg',\n","       'vid_4_30020.jpg', 'vid_4_30440.jpg', 'vid_4_3120.jpg',\n","       'vid_4_3160.jpg', 'vid_4_3140.jpg', 'vid_4_3180.jpg',\n","       'vid_4_3200.jpg', 'vid_4_3220.jpg', 'vid_4_3240.jpg',\n","       'vid_4_3260.jpg', 'vid_4_3380.jpg', 'vid_4_3360.jpg',\n","       'vid_4_3340.jpg', 'vid_4_3440.jpg', 'vid_4_3460.jpg',\n","       'vid_4_3520.jpg', 'vid_4_3540.jpg', 'vid_4_3560.jpg',\n","       'vid_4_3820.jpg', 'vid_4_3800.jpg', 'vid_4_3840.jpg',\n","       'vid_4_4540.jpg', 'vid_4_4520.jpg', 'vid_4_4560.jpg',\n","       'vid_4_600.jpg', 'vid_4_6160.jpg', 'vid_4_6180.jpg',\n","       'vid_4_6200.jpg', 'vid_4_620.jpg', 'vid_4_6220.jpg',\n","       'vid_4_6240.jpg', 'vid_4_6260.jpg', 'vid_4_6280.jpg',\n","       'vid_4_6320.jpg', 'vid_4_6300.jpg', 'vid_4_6340.jpg',\n","       'vid_4_6360.jpg', 'vid_4_6380.jpg', 'vid_4_6400.jpg',\n","       'vid_4_6420.jpg', 'vid_4_6440.jpg', 'vid_4_6460.jpg',\n","       'vid_4_6480.jpg', 'vid_4_6500.jpg', 'vid_4_6520.jpg',\n","       'vid_4_680.jpg', 'vid_4_700.jpg', 'vid_4_720.jpg', 'vid_4_740.jpg',\n","       'vid_4_8240.jpg', 'vid_4_8220.jpg', 'vid_4_8260.jpg',\n","       'vid_4_8280.jpg', 'vid_4_8320.jpg', 'vid_4_8340.jpg',\n","       'vid_4_8300.jpg', 'vid_4_8560.jpg', 'vid_4_860.jpg',\n","       'vid_4_8580.jpg', 'vid_4_8600.jpg', 'vid_4_8640.jpg',\n","       'vid_4_8660.jpg', 'vid_4_8680.jpg', 'vid_4_8700.jpg',\n","       'vid_4_8720.jpg', 'vid_4_8740.jpg', 'vid_4_880.jpg',\n","       'vid_4_8960.jpg', 'vid_4_8980.jpg', 'vid_4_9000.jpg',\n","       'vid_4_900.jpg', 'vid_4_9020.jpg', 'vid_4_9040.jpg',\n","       'vid_4_9060.jpg', 'vid_4_9080.jpg', 'vid_4_920.jpg',\n","       'vid_4_9200.jpg', 'vid_4_9220.jpg', 'vid_4_9240.jpg',\n","       'vid_4_9260.jpg', 'vid_4_9280.jpg', 'vid_4_9300.jpg',\n","       'vid_4_9320.jpg', 'vid_4_9340.jpg', 'vid_4_940.jpg',\n","       'vid_4_9420.jpg', 'vid_4_9460.jpg', 'vid_4_9440.jpg',\n","       'vid_4_9500.jpg', 'vid_4_9520.jpg', 'vid_4_9540.jpg',\n","       'vid_4_9560.jpg', 'vid_4_9580.jpg', 'vid_4_960.jpg',\n","       'vid_4_9600.jpg', 'vid_4_9640.jpg', 'vid_4_9660.jpg',\n","       'vid_4_9620.jpg', 'vid_4_9720.jpg', 'vid_4_9760.jpg',\n","       'vid_4_9740.jpg', 'vid_4_9700.jpg', 'vid_4_9780.jpg',\n","       'vid_4_980.jpg', 'vid_4_9800.jpg', 'vid_4_9820.jpg',\n","       'vid_4_9840.jpg', 'vid_4_9860.jpg', 'vid_4_9880.jpg',\n","       'vid_4_9900.jpg', 'vid_4_9960.jpg', 'vid_4_9980.jpg'], dtype=object)"]},"metadata":{},"execution_count":5}],"source":["image_unique = dataset.image.unique()\n","image_unique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWArDj8gx-jx"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as T\n","from PIL import Image\n","import os\n","\n","class Car_data(Dataset):\n","  def __init__(self, data, image_unique, labels):\n","    self.unique = image_unique\n","    self.data = data\n","    self.labels = labels\n","  def __len__(self):\n","    return len(self.unique)\n","  def __getitem__(self, idx):\n","    image_name = self.unique[idx]\n","    img = Image.open(os.path.join(self.data, image_name)).convert('RGB')\n","    boxes = self.labels[(image_name == self.labels['image'])].values[:, 1:].astype('float')\n","    labels = torch.ones((boxes.shape[0]), dtype = torch.int64)\n","    target = {}\n","    target['boxes'] = torch.tensor(boxes)\n","    target['label'] = labels\n","    return T.ToTensor()(img), target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMgqgeXPyaIt"},"outputs":[],"source":["def custom_collate(data):\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9FdZvzq3S34"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_val = train_test_split(image_unique, test_size = 0.2, random_state= 42)\n","train_data = Car_data('/content/data/training_images', x_train, dataset)\n","val_data = Car_data('/content/data/training_images', x_val, dataset)\n","train_loader = DataLoader(train_data, batch_size = 1, shuffle = True, collate_fn = custom_collate)\n","val_loader = DataLoader(val_data, batch_size = 1, shuffle = True, collate_fn = custom_collate)"]},{"cell_type":"markdown","metadata":{"id":"z__nlU8f6mnW"},"source":["# **Build Faster-R-CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlhsCFtjqGR_"},"outputs":[],"source":["from torchvision.models.detection import fasterrcnn_resnet50_fpn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHWBBliCBKh0"},"outputs":[],"source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","model = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1')\n","num_classes = 2\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rw8hrA6v0jC2"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"5n6JsGNRBfEd"},"source":["# **Train Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FknRx9ZK1RXW"},"outputs":[],"source":["!pip install torchmetrics[detection]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ij7OWc5T6tKF"},"outputs":[],"source":["from torch import optim\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.0005)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","def train(model, dataloader, epochs, optimizer, val_loader):\n","  losses = []\n","  best_loss = 0\n","  best_ap = 0\n","  mAps = []\n","  for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0\n","    loop = tqdm(enumerate(dataloader), total = len(dataloader))\n","    for idx, data in loop:\n","      imgs = []\n","      targets = []\n","      for d in data:\n","        imgs.append(d[0].to(device))\n","        targ = {}\n","        targ['boxes'] = d[1]['boxes'].to(device)\n","        targ['labels'] = d[1]['label'].to(device)\n","        targets.append(targ)\n","      loss_dict = model(imgs, targets)\n","      loss = sum(v for v in loss_dict.values())\n","      epoch_loss += loss.cpu().detach().numpy()\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()  \n","    map = validate(model, val_loader)\n","    mAps.append(map)\n","    loop.set_description(f\"mAp: {map}\")\n","    loop.set_postfix(loss = epoch_loss)\n","    losses.append(epoch_loss)\n","    if map > best_ap:\n","      best_ap = map\n","      torch.save({\n","          \"model_state_dict\": model.state_dict(),\n","          \"optimizer_state_dict\": optimizer.state_dict()\n","      }, 'best_model.pt')\n","  return losses, mAps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62AG1aDHFy2l"},"outputs":[],"source":["from torchvision import ops\n","from torchmetrics.detection import mean_ap\n","\n","def meanAveragePrecision(predict, target):\n","  metric = mean_ap.MeanAveragePrecision()\n","  metric.update(predict, target)\n","  x = metric.compute()\n","  mAp = x['map'].numpy()\n","  return mAp\n","\n","def validate(model, dataloader):\n","  model.eval()\n","  mAp = 0\n","  with torch.no_grad():\n","    for data in dataloader:\n","      target = []\n","      imgs = []\n","      for d in data:\n","        imgs.append(d[0].to(device))\n","        targ = {}\n","        targ['boxes'] = d[1]['boxes'].to(device)\n","        targ['labels'] = d[1]['label'].to(device)\n","        target.append(targ)\n","      output = model(imgs)\n","      mAp += meanAveragePrecision(output, target)\n","  return mAp / len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJP1dqsY56IJ"},"outputs":[],"source":["loss, mAp = train(model, train_loader, 30, optimizer, val_loader)"]},{"cell_type":"markdown","metadata":{"id":"IjLfzYh-NdXU"},"source":["# **Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pQe0sHKNrg0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680900918632,"user_tz":180,"elapsed":2892,"user":{"displayName":"Marcelo Barreto","userId":"14781669852484137175"}},"outputId":"45f77c71-1c28-43e9-9a77-514b488f47b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-04-07 20:55:15--  https://app.roboflow.com/ds/s4gd3l9oss?key=pxE4Cs4MS4\n","Resolving app.roboflow.com (app.roboflow.com)... 151.101.65.195, 151.101.1.195\n","Connecting to app.roboflow.com (app.roboflow.com)|151.101.65.195|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://storage.googleapis.com/roboflow-platform-exports/ueF6mYObchVdiLSL6TW20tlvMoc2/9gZnTjSTwYamLUdfxx4G/4/yolov8.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20230407%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230407T205515Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=7d294d9ded89296f5c846160fcfe23051af383338dd65c0f8520567a9ecb05bdf008b4d167a7671d24c0066ebbcc56f6425b0045f3565db2b3432c7e6261387ce02a099487d1e2537870755b254ebefeb6f72aa34d10b37a9384cef224f96b3b7961210fdc3962dc94933f4aa1f92c0c8a0839a819dd46da1f1084f0a87e6b181ee4a4727c4c45e2d6e209e9692d6df1f1e57142fb49880ada24eba3b704ffaef8fcd6eb87a415668363dbb0b8e0333f84e0bfc7bc5f8db55c5b551f0587d3a812cbf85d7731130c04f936d3c8c9b525f1d16ceb4a224b2bdf7cb69e4f4c3d021fe5bd8a4af4bc183d195557c085d8d825a8c48068f7ddbf60abb1256f759f7d [following]\n","--2023-04-07 20:55:15--  https://storage.googleapis.com/roboflow-platform-exports/ueF6mYObchVdiLSL6TW20tlvMoc2/9gZnTjSTwYamLUdfxx4G/4/yolov8.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20230407%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230407T205515Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=7d294d9ded89296f5c846160fcfe23051af383338dd65c0f8520567a9ecb05bdf008b4d167a7671d24c0066ebbcc56f6425b0045f3565db2b3432c7e6261387ce02a099487d1e2537870755b254ebefeb6f72aa34d10b37a9384cef224f96b3b7961210fdc3962dc94933f4aa1f92c0c8a0839a819dd46da1f1084f0a87e6b181ee4a4727c4c45e2d6e209e9692d6df1f1e57142fb49880ada24eba3b704ffaef8fcd6eb87a415668363dbb0b8e0333f84e0bfc7bc5f8db55c5b551f0587d3a812cbf85d7731130c04f936d3c8c9b525f1d16ceb4a224b2bdf7cb69e4f4c3d021fe5bd8a4af4bc183d195557c085d8d825a8c48068f7ddbf60abb1256f759f7d\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.132.128, 74.125.201.128, 74.125.202.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.132.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 74464498 (71M) [application/zip]\n","Saving to: ‘s4gd3l9oss?key=pxE4Cs4MS4’\n","\n","s4gd3l9oss?key=pxE4 100%[===================>]  71.01M   127MB/s    in 0.6s    \n","\n","2023-04-07 20:55:16 (127 MB/s) - ‘s4gd3l9oss?key=pxE4Cs4MS4’ saved [74464498/74464498]\n","\n","\n","7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n","p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n","\n","Scanning the drive for archives:\n","  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 74464498 bytes (72 MiB)\n","\n","Extracting archive: /content/s4gd3l9oss?key=pxE4Cs4MS4\n","--\n","Path = /content/s4gd3l9oss?key=pxE4Cs4MS4\n","Type = zip\n","Physical Size = 74464498\n","\n","  0%\b\b\b\b    \b\b\b\b 45% 1033 - train/images/vid_4_20100_jpg.r . ea7948e0e03e43dff8a967cc90.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 1522 - train/images/vid_4_8660_jpg.rf.29f998ab136192929d5040a23462eca1.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 2291 - train/labels/vid_4_27060_jpg.r . 8befd8795729fed0d004f10077.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n","\n","Folders: 9\n","Files: 2953\n","Size:       73723554\n","Compressed: 74464498\n"]}],"source":["!wget https://app.roboflow.com/ds/s4gd3l9oss?key=pxE4Cs4MS4\n","!7z x /content/s4gd3l9oss?key=pxE4Cs4MS4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbtNeu-YNq7M"},"outputs":[],"source":["import glob\n","\n","for img in glob.glob('/content/valid/images/*.[jp][pn]g'):\n","  dir_label = img.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","  with open(dir_label, 'r') as f:\n","    lines = f.readlines()\n","    if len(lines) == 0:\n","      os.remove(dir_label)\n","      os.remove(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQ-ivA1vEwO9"},"outputs":[],"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","import glob\n","import numpy as np\n","\n","def get_bboxes(labels):\n","  bboxes = []\n","  with open(labels, 'r') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","      values = line.split(' ')\n","      if len(values) < 4: break\n","      boxes = []\n","      boxes.append(float(values[1]))\n","      boxes.append(float(values[2])) \n","      boxes.append(float(values[3]))\n","      boxes.append(float(values[4].split('\\n')[0]))\n","      bboxes.append(boxes)\n","  return bboxes\n","\n","def desnormalize(bboxes, image):\n","  image = Image.open(image).convert('RGB')\n","  width, height = image.size\n","  for bbox in bboxes:\n","    xmin = bbox[0] - bbox[2]/2\n","    ymin = bbox[1] - bbox[3]/2\n","    xmax = bbox[0] + bbox[2]/2\n","    ymax = bbox[1] + bbox[3]/2\n","    bbox[0] = xmin * width\n","    bbox[1] = ymin * height\n","    bbox[2] = xmax * width\n","    bbox[3] = ymax * height\n","  return bboxes\n","\n","def show_image(image, bboxes, save = False, dir = None):\n","  if not os.path.isdir(dir):\n","    os.mkdir(dir)\n","  img = np.array(Image.open(image).convert('RGB')).astype('float')\n","  for bbox in bboxes:\n","    img = cv2.rectangle(\n","        img,\n","        (int(bbox[0]), int(bbox[1])),\n","        (int(bbox[2]), int(bbox[3])),\n","        color = (255, 0, 0),\n","        thickness = 2\n","    )\n","  if save:\n","    if dir is None:\n","      dir = '/content/show_image'\n","      if not os.path.isdir(dir):\n","          os.mkdir(dir)\n","    name = image.split('/')[-1]\n","    cv2.imwrite(os.path.join(dir, name), img)\n","  else:\n","    cv2.imshow(img)\n","\n","def inference(model, source, save = False, dir = None):\n","  model.eval()\n","  files = glob.glob(source + \"/*.[jp][pn]g\")\n","  results = []\n","  targets = []\n","  mAps = []\n","  for file in files:\n","    image = Image.open(file).convert('RGB')\n","    transform = T.ToTensor()\n","    image = transform(image)\n","    dir_file = file.replace('/images', '/labels').replace('.jpg', '.txt').replace('.png', '.txt')\n","    target = {}\n","    bboxes = get_bboxes(dir_file)\n","    bboxes = desnormalize(bboxes, file)\n","    label = torch.ones(len(bboxes), dtype = torch.int64).to(device)\n","    target['labels'] = label\n","    target['boxes'] = torch.tensor(bboxes).to(device)\n","    targets.append(target)\n","    with torch.no_grad():\n","      output = model([image.to(device)])\n","    '''result = {}\n","    result['boxes'] = output[0]['boxes']\n","    result['labels'] = output[0]['labels']\n","    result['scores'] = output[0]['scores']\n","    result_list = []\n","    target_list = []\n","    result_list.append(result)\n","    target_list.append(target)\n","    mAp = meanAveragePrecision(result_list, target_list)\n","    mAps.append(mAp)\n","    results.append(result)\n","    if save:\n","      if dir is None:\n","        dir = '/content/show_image'\n","        if not os.path.isdir(dir):\n","          os.mkdir(dir)\n","      image_name = file.split('/')[-1]\n","      dir_save = os.path.join(dir, image_name)\n","      os.mkdir(dir_save)\n","      show_image(file, result['boxes'], save = True, dir = f'{dir_save}/inference')\n","      show_image(file, target['boxes'], save = True, dir = f'{dir_save}/ground_truth')\n","      dir_lbl = dir_save + '/' + image_name.replace('.jpg', '.txt').replace('.png', '.txt')\n","      with open(dir_lbl, 'a') as f:\n","        f.writelines(\n","            'Image: ' + ' ' + image_name + '\\n' +\n","            'mAp: ' + ' ' + str(mAp) + '\\n'\n","            'Bboxes: {' + '\\n'\n","        )\n","        for box in result['boxes']:\n","          f.writelines(\n","              ' ' + str(box[0]) + ' ' + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + '\\n'\n","          )\n","        f.writelines('}')'''\n","  return results, targets, mAps\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsXxJQe2meLT"},"outputs":[],"source":["checkpoint = torch.load('/content/best_model.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhKHKITono-D"},"outputs":[],"source":["source = '/content/valid/images'\n","results, targets, mAps = inference(model, source, save = True)"]},{"cell_type":"code","source":["print(results)"],"metadata":{"id":"h70m4bueX20d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mAps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNPCMlx7X7Ct","executionInfo":{"status":"ok","timestamp":1678818404022,"user_tz":180,"elapsed":7,"user":{"displayName":"Rafael Andrade da Silva","userId":"04613237088337635983"}},"outputId":"4decfe9e-34e6-4160-f883-25aada7864b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[array(0.15148515, dtype=float32), array(0.33168316, dtype=float32), array(0.5, dtype=float32), array(0.3, dtype=float32), array(0.36666667, dtype=float32), array(0.4, dtype=float32), array(0.53168315, dtype=float32), array(0.45049506, dtype=float32), array(0., dtype=float32), array(0.1009901, dtype=float32), array(0.5, dtype=float32), array(0.4, dtype=float32), array(0.3, dtype=float32), array(0.3, dtype=float32), array(0.2, dtype=float32), array(0.5, dtype=float32), array(0.3, dtype=float32), array(0.5, dtype=float32), array(0.1009901, dtype=float32), array(0.48118812, dtype=float32), array(0.8, dtype=float32), array(0.4, dtype=float32), array(0.6, dtype=float32), array(0., dtype=float32), array(0.4, dtype=float32), array(0.5, dtype=float32), array(0.4, dtype=float32), array(0.6, dtype=float32), array(0.2, dtype=float32), array(0.4, dtype=float32), array(0.4, dtype=float32), array(0.5, dtype=float32), array(0.4884489, dtype=float32), array(0.42524752, dtype=float32), array(0.32524753, dtype=float32), array(0.23267327, dtype=float32), array(0.5, dtype=float32), array(0., dtype=float32), array(0.9, dtype=float32), array(0.6, dtype=float32), array(0.6, dtype=float32), array(0.5, dtype=float32), array(0.37574258, dtype=float32), array(0., dtype=float32), array(0.5, dtype=float32), array(0.3, dtype=float32), array(0.4009901, dtype=float32), array(0.2, dtype=float32), array(0.5252475, dtype=float32), array(0.26633662, dtype=float32), array(0.6, dtype=float32), array(0.3009901, dtype=float32), array(0.4, dtype=float32), array(0.17574258, dtype=float32), array(0.36435643, dtype=float32), array(0.5, dtype=float32), array(0.05049505, dtype=float32), array(0.42524752, dtype=float32), array(0.5, dtype=float32), array(0.43267328, dtype=float32), array(0.5, dtype=float32), array(0.4, dtype=float32), array(0.1881188, dtype=float32), array(0.5, dtype=float32), array(0.550495, dtype=float32), array(0., dtype=float32), array(0.5, dtype=float32), array(0.4009901, dtype=float32), array(0.4, dtype=float32), array(0.6, dtype=float32), array(0.6, dtype=float32), array(0.5, dtype=float32), array(0.21683168, dtype=float32), array(0.4, dtype=float32), array(0.4, dtype=float32), array(0.25247526, dtype=float32), array(0.03366337, dtype=float32), array(0.6, dtype=float32), array(0.3, dtype=float32), array(0.7, dtype=float32), array(0.1, dtype=float32), array(0.6, dtype=float32), array(0.7, dtype=float32), array(0.4, dtype=float32), array(0.550495, dtype=float32), array(0.25247526, dtype=float32), array(0.18184817, dtype=float32), array(0.6, dtype=float32), array(0.01683168, dtype=float32), array(0.5, dtype=float32)]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["z__nlU8f6mnW","5n6JsGNRBfEd","IjLfzYh-NdXU"],"provenance":[],"machine_shape":"hm"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}