{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j3_t2qz7Vc9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKPdMm_9n3Ki"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!7z x /content/drive/MyDrive/Colab\\ Notebooks/Data\\ Science/Learning/Artificial\\ Neural\\ Networks/PyTorch/dogs-vs-cats.zip\n",
    "!7z x /content/train.zip\n",
    "!7z x /content/test1.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnMln7gJn6pi"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "path = '/content/dogs-vs-cats'\n",
    "os.mkdir(path)\n",
    "os.rename('/content/train', os.path.join(path, 'train'))\n",
    "os.rename('/content/test1', os.path.join(path, 'test1'))\n",
    "os.mkdir(os.path.join(path, 'valid'))\n",
    "files = glob.glob(os.path.join(path, 'train/*.jpg'))\n",
    "no_of_images = len(files)\n",
    "shuffle = np.random.permutation(no_of_images)\n",
    "valid_size = int(3*int(no_of_images / 10))\n",
    "for i in ['train', 'valid']:\n",
    "  for folder in ['dog', 'cat']:\n",
    "    os.mkdir(os.path.join(path, i, folder))\n",
    "for i in shuffle[:valid_size]:\n",
    "  image = files[i].split('/')[-1]\n",
    "  folder = image.split('.')[0]\n",
    "  os.rename(files[i], os.path.join(path, 'valid', folder, image))\n",
    "for i in shuffle[valid_size:]:\n",
    "  image = files[i].split('/')[-1]\n",
    "  folder = image.split('.')[0]\n",
    "  os.rename(files[i], os.path.join(path, 'train', folder, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKpRr9aQBB01"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "train_data = ImageFolder('/content/dogs-vs-cats/train', transform = train_transform)\n",
    "valid_data = ImageFolder('/content/dogs-vs-cats/valid', transform = valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_je2mvhBSSoE"
   },
   "outputs": [],
   "source": [
    "class ImageLoader(Dataset):\n",
    "  def __init__(self, dataset, transform = None):\n",
    "    self.dataset = self.checkChannel(dataset)\n",
    "    self.transform = transform\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  def __getitem__(self, idx):\n",
    "    image = Image.open(self.dataset[idx][0])\n",
    "    classCategory = self.dataset[idx][1]\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, classCategory\n",
    "  def checkChannel(self, dataset):\n",
    "    datasetRGB = []\n",
    "    for index in range(len(dataset)):\n",
    "      if (Image.open(dataset[index][0]).getbands() == ('R', 'G', 'B')):\n",
    "        datasetRGB.append(dataset[index])\n",
    "    return datasetRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHrpTuwBO6KQ"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw_0xdzkWnOE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet50\n",
    "model = resnet50(pretrained = True)\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mfQDEn3PoB6"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "def train(num_epoch, model, train_loader):\n",
    "  for epoch in range(0, num_epoch):\n",
    "    current_loss = 0.0\n",
    "    current_corrects = 0\n",
    "    losses = []\n",
    "    model.train()\n",
    "    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n",
    "    for batch_idx, (data, target) in loop:\n",
    "      data = data.to(device = device)\n",
    "      target = target.to(device = device)\n",
    "      scores = model(data)\n",
    "\n",
    "      loss = criterion(scores, target)\n",
    "      optimizer.zero_grad()\n",
    "      losses.append(loss)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      _, preds = torch.max(scores, 1)\n",
    "      current_loss += loss.item()\n",
    "      current_corrects += (preds == target).sum().item()\n",
    "      accuracy = int(current_corrects / len(train_loader.dataset) * 100)\n",
    "      loop.set_description(f'Epoch {epoch + 1} / {num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}')\n",
    "      loop.set_postfix(loss = loss.data.item())\n",
    "    print(f'Epoch: {epoch} Loss: {current_loss / len(train_loader.dataset)}. Accuracy: {current_corrects / len(train_loader.dataset)}')\n",
    "    torch.save({\n",
    "          'model_state_dict' : model.state_dict(),\n",
    "          'optimizer_state_dict' : optimizer.state_dict(),\n",
    "    }, 'checkpoint_epoch'+str(epoch) +'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhTJv9XPSnrB"
   },
   "outputs": [],
   "source": [
    "def test(model, valid_loader):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for x, y in valid_loader:\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      output = model(x)\n",
    "      _, predictions = torch.max(output, 1)\n",
    "      correct += (predictions == y).sum().item()\n",
    "      test_loss += criterion(output, y)\n",
    "  test_loss /= len(valid_loader.dataset) \n",
    "  print('Average Loss:', test_loss, 'Accuracy:', correct, '/'), len(valid_loader.dataset), ' ', int(correct / len(valid_loader.dataset) * 100 ), '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDglb64RT2CS"
   },
   "outputs": [],
   "source": [
    "train(5, model, train_loader)\n",
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BZ--IO5UGpT"
   },
   "outputs": [],
   "source": [
    "#loading checkpoint\n",
    "checkpoint = torch.load('/content/checkpoint_epoch4.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZB7nnrxUduy"
   },
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "path = '/content/dogs-vs-cats/test1'\n",
    "files = glob.glob(os.path.join(path, '*.jpg'))\n",
    "os.mkdir(os.path.join(path, 'test'))\n",
    "\n",
    "for i in range(0, len(files)):\n",
    "  image = files[i].split('/')[-1]\n",
    "  os.rename(files[i], os.path.join(path, 'test', image))\n",
    "\n",
    "test_dataset = ImageFolder('/content/dogs-vs-cats/test1', transform = test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x8bklc7U6-T"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      _, pred = torch.max(output, 1)\n",
    "      print(f'predict: {pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ_7aAzqtn1H"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "def train(num_epoch, model, train_loader):\n",
    "  for epoch in range(0, num_epoch):\n",
    "    losses = []\n",
    "    current_loss = 0.0\n",
    "    current_accuracy = 0.0\n",
    "    model.train()\n",
    "    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n",
    "    for batch_idx, (data, target) in loop:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(output, target)\n",
    "      _, preds = torch.max(output, 1)\n",
    "      model.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss)\n",
    "      current_loss += loss.item()\n",
    "      current_accuracy += (preds == target).sum().item()\n",
    "      loop.set_description(f'Epoch: {epoch + 1} / {num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}')\n",
    "      loop.set_postfix(loss = loss.data.item())\n",
    "    print(f'Epoch: {epoch} Loss: {current_loss / len(train_loader.dataset)}. Accuracy: {current_accuracy / len(train_loader.dataset)}')\n",
    "    torch.save({\n",
    "        'model_state_dict' : model.state_dict()\n",
    "        'optimizer_state_dict' : optimizer.state_dict()\n",
    "    }, 'checkpoint_epoch' + str(epoch) + '.pt')\n",
    "\n",
    "\n",
    "def valid(num_epoch, model, valid_loader):\n",
    "  model.eval()\n",
    "  loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad:\n",
    "    for data, target in valid_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      _, preds = torch.max(output, 1)\n",
    "      correct += (preds == target).sum().item()\n",
    "  loss = criterion(output, target)\n",
    "\n",
    "checkpoint = torch.load('/content/checkpoint_epoch0.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
